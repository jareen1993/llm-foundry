integrations:
- integration_type: git_repo
  git_repo: jareen1993/llm-foundry
  git_branch: main
  # git_commit: # OR use your commit hash
  pip_install: -e .[gpu]
  # ssh_clone: false # Should be true if using a private repo

command: |
  cd llm-foundry/llmfoundry/inference

  # s3 commands
  pip install awscli
  aws s3 cp --recursive s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/avalaraGPT/model/mpt-30B-pretrained-hf/ local_hf_folder

  # oci commands
  # pip install oci-cli
  # oci os object bulk-download \
  # -bn bucket --region bucket_region \
  # --prefix folder/hf/ --dest-dir ./
  # # oci downloads the full prefix path, this extracts the innermost folder
  # # into local_hf_folder
  # mv folder/hf/ local_hf_folder

  python hf_generate.py \
    --name_or_path local_hf_folder \
    --temperature 1.0 \
    --top_p 0.95 \
    --top_k 50 \
    --seed 1 \
    --max_new_tokens 256 \
    --s3_output_dir "s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/item_prediction/mpt-30B-pretrained-hf/" \
    --prompts \
      "file::s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/item_prediction/test.jsonl"

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
name: hf-generate

compute:
  gpus: 8  # Number of GPUs to use

  ## These configurations are optional
  # cluster: TODO # Name of the cluster to use for this run
  # gpu_type: a100_80gb # Type of GPU to use. We use a100_80gb in our experiments
